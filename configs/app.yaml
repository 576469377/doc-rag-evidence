# Core configuration for V0 doc-rag-evidence system
# Reference: core/schemas.py -> AppConfig

# ===== Path Configuration =====
data_root: "data"
docs_dir: "data/docs"
indices_dir: "data/indices"
runs_dir: "data/runs"
reports_dir: "data/reports"

# ===== Granularity & Retrieval =====
# chunk_level: page | block  (V0 uses block for better precision)
chunk_level: "block"

# Retrieval pipeline
top_k_retrieve: 20      # Initial retrieval candidates
top_k_rerank: 10        # After reranking (if enabled)
top_k_evidence: 5       # Final evidence items for generation

# ===== Citation & Evidence Display =====
# citation_level: page | block (how to cite in answer)
citation_level: "block"

# bbox_mode: none | normalized | absolute_px
# V0 uses "none" - bbox can be added later
bbox_mode: "none"

# ===== Model Configuration =====
# V1.1: Real LLM for generation
generator:
  type: "qwen3_vl"        # template | qwen3_vl (使用qwen3_vl启用真实LLM生成)
  
# LLM Configuration (V1.1+)
llm:
  backend: "vllm"         # transformers | vllm | sglang
  model: "Qwen/Qwen3-VL-4B-Instruct"  # Model name for API (multimodal)
  model_path: "/workspace/cache/Qwen3-VL-4B-Instruct"  # Local model path
  endpoint: "http://localhost:8002"  # vLLM generation server
  gpu: 3                  # GPU device ID
  max_new_tokens: 2048
  temperature: 0.1
  top_p: 0.9
  citation_policy: "strict"  # strict | relaxed | none

# Legacy names (keep for compatibility)
embedder_name: "vllm-embedder"
reranker_name: "none"
llm_name: "qwen3-vl"

# ===== OCR Configuration (V0.1+) =====
ocr:
  provider: "vllm"                  # vllm | sglang | mock
  model: "tencent/HunyuanOCR"       # HunyuanOCR model name
  model_path: "/workspace/cache/HunyuanOCR"  # Local model path
  endpoint: "http://localhost:8000"
  gpu: 0                            # GPU device ID
  timeout: 300
  cache_enabled: true

# ===== Dense Embedding Retrieval (V0.1+) =====
dense:
  enabled: true                     # Enable dense retrieval
  embedder_type: "vllm"             # vllm | sglang embedding API
  model: "Qwen/Qwen3-Embedding-0.6B"  # Model name
  model_path: "/workspace/cache/Qwen3-Embedding-0.6B"  # Local model path
  endpoint: "http://localhost:8001" # vllm server endpoint
  gpu: 1                            # GPU device ID
  index_type: "Flat"                # Flat | IVF
  nlist: 100                        # IVF clusters
  nprobe: 10                        # IVF search clusters
  batch_size: 32

# ===== ColPali Vision Retrieval (V0.1+) =====
colpali:
  enabled: true                     # Enable vision retrieval
  model: "/workspace/cache/tomoro-colqwen3-embed-4b"  # ColQwen3 local model
  device: "cuda:2"                  # CUDA device for model
  gpu: 2                            # GPU device ID
  max_global_pool: 100              # Coarse stage top-N
  cache_dir: "data/cache/colpali"   # Embedding cache

# ===== Retrieval Mode (V0.1+) =====
# retrieval_mode: bm25 | dense | colpali | hybrid
retrieval_mode: "bm25"              # Default to BM25 baseline

# ===== Context & Safety =====
max_context_chars: 12000            # Hard cap for LLM context
require_citations: true             # Force citation output

# ===== Data Artifact Path Rules =====
# Documents: data/docs/{doc_id}/meta.json
# Pages: data/docs/{doc_id}/pages/{page_id:04d}/text.json
# Blocks: data/docs/{doc_id}/pages/{page_id:04d}/blocks.json
# Page images: data/docs/{doc_id}/pages/{page_id:04d}/page.png (optional)
# Indices: data/indices/{index_name}/* 
# Run logs: data/runs/{query_id}.json
# Eval reports: data/reports/{dataset}/{timestamp}/report.json + predictions.csv
